{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item features shape: (10, 16)\n",
      "User features shape: (5000, 128)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the item data\n",
    "item_data_path = 'data/sushi3-2016/sushi3.idata'\n",
    "item_columns = ['item_id', 'name', 'style', 'major_group', 'minor_group', 'heaviness', 'consumption_frequency', 'normalized_price', 'sell_frequency']\n",
    "item_df = pd.read_csv(item_data_path, sep='\\t', header=None, names=item_columns)\n",
    "\n",
    "# Filter the item data to include only the 10 items used in the paper\n",
    "item_set_A_ids = [0, 1, 2, 3, 4, 6, 7, 8, 26, 29]\n",
    "#item_set_A_ids=[i for i in range(100)]\n",
    "item_set_A_df = item_df[item_df['item_id'].isin(item_set_A_ids)]\n",
    "\n",
    "# Preprocess the item features\n",
    "categorical_features = ['style', 'major_group', 'minor_group']\n",
    "numerical_features = ['heaviness', 'consumption_frequency', 'normalized_price', 'sell_frequency']\n",
    "\n",
    "# Convert categorical features to strings to ensure get_dummies works correctly\n",
    "\n",
    "item_features = pd.get_dummies(item_set_A_df[categorical_features].astype(str))\n",
    "item_features = pd.concat([item_features, item_set_A_df[numerical_features]], axis=1)\n",
    "\n",
    "# Display the preprocessed item features\n",
    "print(\"Item features shape:\", item_features.shape)\n",
    "\n",
    "# Load the user data\n",
    "user_data_path = 'data/sushi3-2016/sushi3.udata'\n",
    "user_columns = ['user_id', 'gender', 'age', 'total_time', 'prefecture_longest', 'region_longest', 'east_west_longest', 'prefecture_current', 'region_current', 'east_west_current', 'prefecture_diff']\n",
    "user_df = pd.read_csv(user_data_path, sep='\\t', header=None, names=user_columns)\n",
    "\n",
    "# Preprocess the user features\n",
    "categorical_features_user = ['gender', 'age', 'prefecture_longest', 'region_longest', 'east_west_longest', 'prefecture_current', 'region_current', 'east_west_current']\n",
    "\n",
    "# Convert categorical features to strings to ensure get_dummies works correctly\n",
    "user_df[categorical_features_user] = user_df[categorical_features_user].astype(str)\n",
    "\n",
    "user_features = pd.get_dummies(user_df[categorical_features_user])\n",
    "\n",
    "# Display the preprocessed user features\n",
    "print(\"User features shape:\", user_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o arquivo de preferencias e removendo as duas primeiras colunas de metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and parse the preference order data manually\n",
    "preference_data_path = 'data/sushi3-2016/sushi3a.5000.10.order'\n",
    "with open(preference_data_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove the first row which contains metadata\n",
    "lines = lines[1:]\n",
    "\n",
    "# Split each line into a list of preferences\n",
    "preference_data = [line.strip().split() for line in lines]\n",
    "\n",
    "# Convert to a DataFrame\n",
    "preference_df = pd.DataFrame(preference_data)\n",
    "\n",
    "# Convert all values to integers\n",
    "preference_df = preference_df.astype(int)\n",
    "\n",
    "# Rename columns for clarity\n",
    "preference_df.columns = [f'pref_{i}' for i in range(preference_df.shape[1])]\n",
    "\n",
    "# Display the processed preference data\n",
    "\n",
    "# Remove the first two columns if they contain metadata\n",
    "preference_df = preference_df.drop(columns=['pref_0', 'pref_1'])\n",
    "\n",
    "# Rename columns for clarity\n",
    "preference_df.columns = [f'pref_{i}' for i in range(preference_df.shape[1])]\n",
    "\n",
    "# Display the cleaned preference data\n",
    "print(preference_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPPrefenceElicitation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, user_features, item_features, preference_data, max_iter=100):\n",
    "        self.user_features = user_features\n",
    "        self.item_features = item_features\n",
    "        self.preference_data = preference_data\n",
    "        self.n_users = user_features.shape[0]\n",
    "        self.n_items = item_features.shape[0]\n",
    "        self.maximum_hyperparameters()\n",
    "        user_covariance = self.kernel_covariance_matrix(user_features, user_features, self.sv_t, self.ls_t)\n",
    "        item_covariance = self.kernel_covariance_matrix(item_features, item_features, self.sv_k, self.ls_k)\n",
    "        self.covariance_matrix = np.kron(user_covariance, item_covariance)\n",
    "        self.precision_matrix = np.linalg.inv(self.covariance_matrix)\n",
    "    \n",
    "    def grad_log_p_D_given_f(self, f, preference_data):\n",
    "        grad = np.zeros_like(f)\n",
    "        \n",
    "        for preference in preference_data:\n",
    "            t, k_1, k_2 = preference\n",
    "            index_1 = t * self.n_users + k_1\n",
    "            index_2 = t * self.n_users + k_2\n",
    "            z = (f[index_1] - f[index_2]) / self.noise_sd\n",
    "            phi_val = scipy.stats.norm.pdf(z)\n",
    "            Phi_val = scipy.stats.norm.cdf(z)\n",
    "            ratio = phi_val / (Phi_val * self.noise_sd)\n",
    "            grad[index_1] += ratio\n",
    "            grad[index_2] -= ratio\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "    def hessian_log_p_D_given_f(self, f, peference_data):\n",
    "        hessian = np.zeros((f.shape[0], f.shape[0]))\n",
    "        \n",
    "        for preference in preference_data:\n",
    "            t, k_1, k_2 = preference\n",
    "            index_1 = t * self.n_users + k_1\n",
    "            index_2 = t * self.n_users + k_2\n",
    "            z = (f[index_1] - f[index_2]) / self.noise_sd\n",
    "            phi_val = scipy.stats.norm.pdf(z)\n",
    "            Phi_val = scipy.stats.norm.cdf(z)\n",
    "            ratio = phi_val / (Phi_val * self.noise_sd)\n",
    "            \n",
    "            second_derivative = (z * ratio + ratio**2) / (self.noise_sd ** 2)\n",
    "            hessian[index_1, index_1] -= second_derivative\n",
    "            hessian[index_2, index_2] -= second_derivative\n",
    "            hessian[index_1, index_2] += second_derivative\n",
    "            hessian[index_2, index_1] += second_derivative\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def maximum_a_posteriori(self, initial_f, preference_data, max_iter=100):\n",
    "        f = initial_f\n",
    "        for i in range(max_iter):\n",
    "            hessian = self.hessian_log_p_D_given_f(f, preference_data)\n",
    "            precision_matrix_post = hessian + self.precision_matrix\n",
    "            grad = self.grad_log_p_D_given_f(f, preference_data)\n",
    "            hessian_mult_f = hessian @ f\n",
    "            f = np.linalg.solve(precision_matrix_post, grad + hessian_mult_f)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def se_kernel(self, x, y, sv, ls):\n",
    "        return sv * np.exp(-0.5 * np.linalg.norm(x - y) ** 2 / ls ** 2)\n",
    "    \n",
    "    def kernel_covariance_matrix(self, x, y, sv, ls):\n",
    "        n_x = x.shape[0]\n",
    "        n_y = y.shape[0]\n",
    "        covariance_matrix = np.zeros((n_x, n_y))\n",
    "        \n",
    "        for i in range(n_x):\n",
    "            for j in range(n_y):\n",
    "                covariance_matrix[i, j] = self.se_kernel(x[i], y[j], sv, ls)\n",
    "        \n",
    "        return covariance_matrix\n",
    "\n",
    "    def maximum_hyperparameters(self, init_sv_t=1, init_ls_t=1, init_sv_k=1, init_ls_k=1, init_noise_sd=0.1, max_iter=100):\n",
    "        sv_t = torch.tensor(init_sv_t, requires_grad=True)\n",
    "        ls_t = torch.tensor(init_ls_t, requires_grad=True)\n",
    "        sv_k = torch.tensor(init_sv_k, requires_grad=True)\n",
    "        ls_k = torch.tensor(init_ls_k, requires_grad=True)\n",
    "        self.noise_sd = torch.tensor(init_noise_sd, requires_grad=True)\n",
    "        optimizer = torch.optim.Adam([sv_t, ls_t, sv_k, ls_k, self.noise_sd])\n",
    "        identity = torch.eye(self.n_users * self.n_items)\n",
    "        for i in range(max_iter):\n",
    "            optimizer.zero_grad()\n",
    "            user_covariance = self.kernel_covariance_matrix(self.user_features, self.user_features, sv_t, ls_t)\n",
    "            item_covariance = self.kernel_covariance_matrix(self.item_features, self.item_features, sv_k, ls_k)\n",
    "            self.covariance_matrix = np.kron(user_covariance, item_covariance)\n",
    "            self.precision_matrix = torch.inverse(torch.tensor(self.covariance_matrix))\n",
    "            \n",
    "            # Remove no_grad()?\n",
    "            with torch.no_grad():\n",
    "                f = self.maximum_a_posteriori(np.zeros(self.n_users * self.n_items))\n",
    "                hessian = self.hessian_log_p_D_given_f(f, self.preference_data)\n",
    "                grad = self.grad_log_p_D_given_f(f, self.preference_data)\n",
    "            \n",
    "            loss = 0.5 * torch.logdet(self.covariance_matrix @ hessian + identity) + 0.5 * f @ self.precision_matrix @ f - grad\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        self.sv_t = sv_t.item()\n",
    "        self.ls_t = ls_t.item()\n",
    "        self.sv_k = sv_k.item()\n",
    "        self.ls_k = ls_k.item()\n",
    "        self.noise_sd = self.noise_sd.item()\n",
    "    \n",
    "    def predictive_mean_and_variance(self, f_hat, hessian, t_features, x_features):\n",
    "        k_t_star = torch.zeros(self.n_users, 1)\n",
    "        for i in range(self.n_users):\n",
    "            k_t_star[i] = self.se_kernel(t_features, self.user_features[i], self.sv_t, self.ls_t)\n",
    "        \n",
    "        k_x_star = torch.zeros(self.n_items, 1)\n",
    "        for i in range(self.n_items):\n",
    "            k_x_star[i] = self.se_kernel(x_features, self.item_features[i], self.sv_k, self.ls_k)\n",
    "        \n",
    "        kernel_t = self.kernel_covariance_matrix(t_features, t_features, self.sv_t, self.ls_t)\n",
    "        kernel_x = self.kernel_covariance_matrix(x_features, x_features, self.sv_k, self.ls_k)\n",
    "        Sigma_star = kernel_t * kernel_x\n",
    "        \n",
    "        k_star = torch.kron(k_t_star, k_x_star)\n",
    "        \n",
    "        predictive_mean = k_star.T @ self.precision_matrix @ f_hat\n",
    "        predictive_variance = Sigma_star - k_star.T @ torch.inverse(self.covariance_matrix + torch.inverse(hessian)) @ k_star\n",
    "        \n",
    "        return predictive_mean, predictive_variance\n",
    "    \n",
    "    def joint_predictive_mean_and_variance(self, f_hat, hessian, t_features, x_1_features, x_2_features):\n",
    "        k_t_star = torch.zeros(self.n_users, 1)\n",
    "        for i in range(self.n_users):\n",
    "            k_t_star[i] = self.se_kernel(t_features, self.user_features[i], self.sv_t, self.ls_t)\n",
    "        \n",
    "        k_x_star = torch.zeros(self.n_items, 2)\n",
    "        for i in range(self.n_items):\n",
    "            k_x_star[i, 0] = self.se_kernel(x_1_features, self.item_features[i], self.sv_k, self.ls_k)\n",
    "            k_x_star[i, 1] = self.se_kernel(x_2_features, self.item_features[i], self.sv_k, self.ls_k)\n",
    "        \n",
    "        kernel_t = self.kernel_covariance_matrix(t_features, t_features, self.sv_t, self.ls_t)\n",
    "        Sigma_star = torch.zeros(2, 2)\n",
    "        Sigma_star[0, 0] = kernel_t * self.kernel_covariance_matrix(x_1_features, x_1_features, self.sv_k, self.ls_k)\n",
    "        Sigma_star[1, 1] = kernel_t * self.kernel_covariance_matrix(x_2_features, x_2_features, self.sv_k, self.ls_k)\n",
    "        Sigma_star[0, 1] = kernel_t * self.kernel_covariance_matrix(x_1_features, x_2_features, self.sv_k, self.ls_k)\n",
    "        Sigma_star[1, 0] = kernel_t * self.kernel_covariance_matrix(x_2_features, x_1_features, self.sv_k, self.ls_k)\n",
    "        \n",
    "        k_star = torch.kron(k_t_star, k_x_star)\n",
    "        \n",
    "        predictive_mean = k_star.T @ self.precision_matrix @ f_hat\n",
    "        predictive_variance = Sigma_star - k_star.T @ torch.inverse(self.covariance_matrix + torch.inverse(hessian)) @ k_star\n",
    "        \n",
    "        return predictive_mean, predictive_variance\n",
    "    \n",
    "    def expected_improvement(self, t_features, x_features, f_hat, hessian, f_best=None):\n",
    "        if f_best is None:\n",
    "            f_best = f_hat\n",
    "        \n",
    "        predictive_mean, predictive_variance = self.predictive_mean_and_variance(f_hat, hessian, t_features, x_features)\n",
    "        \n",
    "        pred_sd = torch.sqrt(predictive_variance)\n",
    "        z_prime = (predictive_mean - f_best) / pred_sd\n",
    "        \n",
    "        return pred_sd * (z_prime * scipy.stats.norm.cdf(z_prime) + scipy.stats.norm.pdf(z_prime))\n",
    "    \n",
    "    def maximum_expected_improvement(self, t_features, item_features, f_hat, hessian, f_best=None):\n",
    "        maximum_ei = -np.inf\n",
    "        \n",
    "        for x in item_features:\n",
    "            ei = self.expected_improvement(t_features, x, f_hat, hessian, f_best)\n",
    "            if ei > maximum_ei:\n",
    "                maximum_ei = ei\n",
    "        \n",
    "        return maximum_ei\n",
    "    \n",
    "    def prob_i_over_j(self, t_features, x_1_index, x_2_index, f_hat, hessian):\n",
    "        predictive_mean, predictive_variance = self.joint_predictive_mean_and_variance(f_hat, hessian, t_features, self.item_features[x_1_index], self.item_features[x_2_index])\n",
    "        \n",
    "        numerator = predictive_mean[0] - predictive_mean[1]\n",
    "        denominator = predictive_variance[0, 0] - predictive_variance[1, 1] - 2 * predictive_variance[0, 1] - self.noise_sd ** 2\n",
    "        \n",
    "        return scipy.stats.norm.cdf(numerator / denominator)\n",
    "\n",
    "    def expected_value_of_information(self, t_index, x_1_index, x_2_index, f_hat, hessian, f_best=None):\n",
    "        t_features = self.user_features[t_index]\n",
    "        maximum_ei = self.maximum_expected_improvement(t_features, self.item_features, f_hat, hessian, f_best)\n",
    "        \n",
    "        preference_data_1_2 = self.preference_data + [[t_index, x_1_index, x_2_index]]\n",
    "        f_hat_1_2 = self.maximum_a_posteriori(f_hat, preference_data_1_2)\n",
    "        hessian_1_2 = self.hessian_log_p_D_given_f(f_hat_1_2, preference_data_1_2)\n",
    "        \n",
    "        preference_data_2_1 = self.preference_data + [[t_index, x_2_index, x_1_index]]\n",
    "        f_hat_2_1 = self.maximum_a_posteriori(f_hat, preference_data_2_1)\n",
    "        hessian_2_1 = self.hessian_log_p_D_given_f(f_hat_2_1, preference_data_2_1)\n",
    "        \n",
    "        mei_1_2 = self.maximum_expected_improvement(t_features, self.item_features, f_hat_1_2, hessian_1_2, f_best)\n",
    "        mei_2_1 = self.maximum_expected_improvement(t_features, self.item_features, f_hat_2_1, hessian_2_1, f_best)\n",
    "        \n",
    "        prob_1_2 = self.prob_i_over_j(t_features, x_1_index, x_2_index, f_hat, hessian)\n",
    "        prob_2_1 = 1 - prob_1_2\n",
    "        \n",
    "        return -maximum_ei + prob_1_2 * mei_1_2 + prob_2_1 * mei_2_1\n",
    "\n",
    "    def preference_elicitation(self, t_index, num_queries):\n",
    "        # Make a copy\n",
    "        pref_data = self.preference_data.copy()\n",
    "        f_hat = torch.zeros(self.n_users * self.n_items)\n",
    "        \n",
    "        for i in range(num_queries):\n",
    "            f_hat = self.maximum_a_posteriori(f_hat, pref_data)\n",
    "            hessian = self.hessian_log_p_D_given_f(f_hat, pref_data)\n",
    "            \n",
    "            max_evoi = -np.inf\n",
    "            for x_1 in range(self.n_items):\n",
    "                for x_2 in range(x_1 + 1, self.n_items):\n",
    "                    evoi = self.expected_value_of_information(t_index, x_1, x_2, f_hat, hessian)\n",
    "                    if evoi > max_evoi:\n",
    "                        max_evoi = evoi\n",
    "                        max_x_1 = x_1\n",
    "                        max_x_2 = x_2\n",
    "            \n",
    "            pref_data.append([t_index, max_x_1, max_x_2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
