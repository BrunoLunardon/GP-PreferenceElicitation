{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the item data\n",
    "item_data_path = 'data/sushi3-2016/sushi3.idata'\n",
    "item_columns = ['item_id', 'name', 'style', 'major_group', 'minor_group', 'heaviness', 'consumption_frequency', 'normalized_price', 'sell_frequency']\n",
    "item_df = pd.read_csv(item_data_path, sep='\\t', header=None, names=item_columns)\n",
    "\n",
    "# Filter the item data to include only the 10 items used in the paper\n",
    "item_set_A_ids = [0, 1, 2, 3, 4, 6, 7, 8, 26, 29]\n",
    "#item_set_A_ids=[i for i in range(100)]\n",
    "item_set_A_df = item_df[item_df['item_id'].isin(item_set_A_ids)]\n",
    "\n",
    "# Preprocess the item features\n",
    "categorical_features = ['style', 'major_group', 'minor_group']\n",
    "numerical_features = ['heaviness', 'consumption_frequency', 'normalized_price', 'sell_frequency']\n",
    "\n",
    "# Convert categorical features to strings to ensure get_dummies works correctly\n",
    "\n",
    "item_features = pd.get_dummies(item_set_A_df[categorical_features].astype(str))\n",
    "item_features = pd.concat([item_features, item_set_A_df[numerical_features]], axis=1)\n",
    "\n",
    "# Display the preprocessed item features\n",
    "print(\"Item features shape:\", item_features.shape)\n",
    "\n",
    "# Load the user data\n",
    "user_data_path = 'data/sushi3-2016/sushi3.udata'\n",
    "user_columns = ['user_id', 'gender', 'age', 'total_time', 'prefecture_longest', 'region_longest', 'east_west_longest', 'prefecture_current', 'region_current', 'east_west_current', 'prefecture_diff']\n",
    "user_df = pd.read_csv(user_data_path, sep='\\t', header=None, names=user_columns)\n",
    "\n",
    "# Preprocess the user features\n",
    "categorical_features_user = ['gender', 'age', 'prefecture_longest', 'region_longest', 'east_west_longest', 'prefecture_current', 'region_current', 'east_west_current']\n",
    "\n",
    "# Convert categorical features to strings to ensure get_dummies works correctly\n",
    "user_df[categorical_features_user] = user_df[categorical_features_user].astype(str)\n",
    "\n",
    "user_features = pd.get_dummies(user_df[categorical_features_user])\n",
    "\n",
    "# Display the preprocessed user features\n",
    "print(\"User features shape:\", user_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o arquivo de preferencias e removendo as duas primeiras colunas de metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and parse the preference order data manually\n",
    "preference_data_path = 'data/sushi3-2016/sushi3a.5000.10.order'\n",
    "with open(preference_data_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove the first row which contains metadata\n",
    "lines = lines[1:]\n",
    "\n",
    "# Split each line into a list of preferences\n",
    "preference_data = [line.strip().split() for line in lines]\n",
    "\n",
    "# Convert to a DataFrame\n",
    "preference_df = pd.DataFrame(preference_data)\n",
    "\n",
    "# Convert all values to integers\n",
    "preference_df = preference_df.astype(int)\n",
    "\n",
    "# Rename columns for clarity\n",
    "preference_df.columns = [f'pref_{i}' for i in range(preference_df.shape[1])]\n",
    "\n",
    "# Display the processed preference data\n",
    "\n",
    "# Remove the first two columns if they contain metadata\n",
    "preference_df = preference_df.drop(columns=['pref_0', 'pref_1'])\n",
    "\n",
    "# Rename columns for clarity\n",
    "preference_df.columns = [f'pref_{i}' for i in range(preference_df.shape[1])]\n",
    "\n",
    "# Display the cleaned preference data\n",
    "print(preference_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pairwise(preference_df):\n",
    "    pairwise_data = []\n",
    "    for user_id, preferences in preference_df.iterrows():\n",
    "        for i in range(len(preferences)):\n",
    "            for j in range(i + 1, len(preferences)):\n",
    "                item_i = preferences[i]\n",
    "                item_j = preferences[j]\n",
    "                pairwise_data.append([user_id, item_i, item_j])\n",
    "    pairwise_df = pd.DataFrame(pairwise_data, columns=['user_id', 'item_i', 'item_j'])\n",
    "    return pairwise_df\n",
    "\n",
    "# Convert the preference data to pairwise format\n",
    "pairwise_df = convert_to_pairwise(preference_df)\n",
    "\n",
    "# Display the processed pairwise preference data\n",
    "print(pairwise_df.head())\n",
    "print(pairwise_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now subsample the pairwise data taking 5 pairs for each of the first 50 users\n",
    "pairwise_df = pairwise_df[pairwise_df['user_id'] < 50]\n",
    "pairwise_df = pairwise_df.sample(5*50, random_state=0)\n",
    "\n",
    "pairwise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPPrefenceElicitation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, user_features, item_features, preference_data, max_iter=100):\n",
    "        self.user_features = user_features\n",
    "        self.item_features = item_features\n",
    "        self.preference_data = preference_data\n",
    "        self.n_users = user_features.shape[0]\n",
    "        self.n_items = item_features.shape[0]\n",
    "        self.maximum_hyperparameters()\n",
    "        user_covariance = self.kernel_covariance_matrix(user_features, user_features, self.sv_t, self.ls_t)\n",
    "        item_covariance = self.kernel_covariance_matrix(item_features, item_features, self.sv_k, self.ls_k)\n",
    "        self.covariance_matrix = np.kron(user_covariance, item_covariance)\n",
    "        self.precision_matrix = np.linalg.inv(self.covariance_matrix)\n",
    "    \n",
    "    def grad_log_p_D_given_f(self, f):\n",
    "        grad = np.zeros_like(f)\n",
    "        \n",
    "        for preference in self.preference_data:\n",
    "            t, k_1, k_2 = preference\n",
    "            index_1 = t * self.n_users + k_1\n",
    "            index_2 = t * self.n_users + k_2\n",
    "            z = (f[index_1] - f[index_2]) / self.noise_sd\n",
    "            phi_val = scipy.stats.norm.pdf(z)\n",
    "            Phi_val = scipy.stats.norm.cdf(z)\n",
    "            ratio = phi_val / (Phi_val * self.noise_sd)\n",
    "            grad[index_1] += ratio\n",
    "            grad[index_2] -= ratio\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "    def hessian_log_p_D_given_f(self, f):\n",
    "        hessian = np.zeros((f.shape[0], f.shape[0]))\n",
    "        \n",
    "        for preference in self.preference_data:\n",
    "            t, k_1, k_2 = preference\n",
    "            index_1 = t * self.n_users + k_1\n",
    "            index_2 = t * self.n_users + k_2\n",
    "            z = (f[index_1] - f[index_2]) / self.noise_sd\n",
    "            phi_val = scipy.stats.norm.pdf(z)\n",
    "            Phi_val = scipy.stats.norm.cdf(z)\n",
    "            ratio = phi_val / (Phi_val * self.noise_sd)\n",
    "            \n",
    "            second_derivative = (z * ratio + ratio**2) / (self.noise_sd ** 2)\n",
    "            hessian[index_1, index_1] -= second_derivative\n",
    "            hessian[index_2, index_2] -= second_derivative\n",
    "            hessian[index_1, index_2] += second_derivative\n",
    "            hessian[index_2, index_1] += second_derivative\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def maximum_a_posteriori(self, initial_f, max_iter=100):\n",
    "        f = initial_f\n",
    "        for i in range(max_iter):\n",
    "            hessian = self.hessian_log_p_D_given_f(f)\n",
    "            precision_matrix_post = hessian + self.precision_matrix\n",
    "            grad = self.grad_log_p_D_given_f(f)\n",
    "            hessian_mult_f = hessian @ f\n",
    "            f = np.linalg.solve(precision_matrix_post, grad + hessian_mult_f)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def se_kernel(self, x, y, sv, ls):\n",
    "        return sv * np.exp(-0.5 * np.linalg.norm(x - y) ** 2 / ls ** 2)\n",
    "    \n",
    "    def kernel_covariance_matrix(self, x, y, sv, ls):\n",
    "        n_x = x.shape[0]\n",
    "        n_y = y.shape[0]\n",
    "        covariance_matrix = np.zeros((n_x, n_y))\n",
    "        \n",
    "        for i in range(n_x):\n",
    "            for j in range(n_y):\n",
    "                covariance_matrix[i, j] = self.se_kernel(x[i], y[j], sv, ls)\n",
    "        \n",
    "        return covariance_matrix\n",
    "\n",
    "    def maximum_hyperparameters(self, init_sv_t=1, init_ls_t=1, init_sv_k=1, init_ls_k=1, init_noise_sd=0.1, max_iter=100):\n",
    "        sv_t = torch.tensor(init_sv_t, requires_grad=True)\n",
    "        ls_t = torch.tensor(init_ls_t, requires_grad=True)\n",
    "        sv_k = torch.tensor(init_sv_k, requires_grad=True)\n",
    "        ls_k = torch.tensor(init_ls_k, requires_grad=True)\n",
    "        self.noise_sd = torch.tensor(init_noise_sd, requires_grad=True)\n",
    "        optimizer = torch.optim.Adam([sv_t, ls_t, sv_k, ls_k, self.noise_sd])\n",
    "        identity = torch.eye(self.n_users * self.n_items)\n",
    "        for i in range(max_iter):\n",
    "            optimizer.zero_grad()\n",
    "            user_covariance = self.kernel_covariance_matrix(self.user_features, self.user_features, sv_t, ls_t)\n",
    "            item_covariance = self.kernel_covariance_matrix(self.item_features, self.item_features, sv_k, ls_k)\n",
    "            self.covariance_matrix = np.kron(user_covariance, item_covariance)\n",
    "            self.precision_matrix = torch.inverse(torch.tensor(self.covariance_matrix))\n",
    "            \n",
    "            # Remove no_grad()?\n",
    "            with torch.no_grad():\n",
    "                f = self.maximum_a_posteriori(np.zeros(self.n_users * self.n_items))\n",
    "                hessian = self.hessian_log_p_D_given_f(f)\n",
    "                grad = self.grad_log_p_D_given_f(f)\n",
    "            \n",
    "            loss = 0.5 * torch.logdet(self.covariance_matrix @ hessian + identity) + 0.5 * f @ self.precision_matrix @ f - grad\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        self.sv_t = sv_t.item()\n",
    "        self.ls_t = ls_t.item()\n",
    "        self.sv_k = sv_k.item()\n",
    "        self.ls_k = ls_k.item()\n",
    "        self.noise_sd = self.noise_sd.item()\n",
    "    \n",
    "    def predictive_mean_and_variance(self, user_features, item_features):\n",
    "        user_covariance = self.kernel_covariance_matrix(user_features, self.user_features, self.sv_t, self.ls_t)\n",
    "        item_covariance = self.kernel_covariance_matrix(item_features, self.item_features, self.sv_k, self.ls_k)\n",
    "        covariance_matrix = np.kron(user_covariance, item_covariance)\n",
    "        predictive_mean = item_covariance @ self.precision_matrix @ self.preference_data\n",
    "        predictive_variance = item_covariance - item_covariance @ self.precision_matrix @ item_covariance\n",
    "        \n",
    "        return predictive_mean, predictive_variance\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
