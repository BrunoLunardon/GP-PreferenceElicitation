{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the item data\n",
    "item_data_path = 'data/sushi3-2016/sushi3.idata'\n",
    "item_columns = ['item_id', 'name', 'style', 'major_group', 'minor_group', 'heaviness', 'consumption_frequency', 'normalized_price', 'sell_frequency']\n",
    "item_df = pd.read_csv(item_data_path, sep='\\t', header=None, names=item_columns)\n",
    "\n",
    "# Filter the item data to include only the 10 items used in the paper\n",
    "item_set_A_ids = [0, 1, 2, 3, 4, 6, 7, 8, 26, 29]\n",
    "#item_set_A_ids=[i for i in range(100)]\n",
    "item_set_A_df = item_df[item_df['item_id'].isin(item_set_A_ids)]\n",
    "\n",
    "# Preprocess the item features\n",
    "categorical_features = ['style', 'major_group', 'minor_group']\n",
    "numerical_features = ['heaviness', 'consumption_frequency', 'normalized_price', 'sell_frequency']\n",
    "\n",
    "# Convert categorical features to strings to ensure get_dummies works correctly\n",
    "\n",
    "item_features = pd.get_dummies(item_set_A_df[categorical_features].astype(str))\n",
    "item_features = pd.concat([item_features, item_set_A_df[numerical_features]], axis=1)\n",
    "\n",
    "# Display the preprocessed item features\n",
    "print(\"Item features shape:\", item_features.shape)\n",
    "\n",
    "# Load the user data\n",
    "user_data_path = 'data/sushi3-2016/sushi3.udata'\n",
    "user_columns = ['user_id', 'gender', 'age', 'total_time', 'prefecture_longest', 'region_longest', 'east_west_longest', 'prefecture_current', 'region_current', 'east_west_current', 'prefecture_diff']\n",
    "user_df = pd.read_csv(user_data_path, sep='\\t', header=None, names=user_columns)\n",
    "\n",
    "# Preprocess the user features\n",
    "categorical_features_user = ['gender', 'age', 'prefecture_longest', 'region_longest', 'east_west_longest', 'prefecture_current', 'region_current', 'east_west_current']\n",
    "\n",
    "# Convert categorical features to strings to ensure get_dummies works correctly\n",
    "user_df[categorical_features_user] = user_df[categorical_features_user].astype(str)\n",
    "\n",
    "user_features = pd.get_dummies(user_df[categorical_features_user])\n",
    "\n",
    "# Display the preprocessed user features\n",
    "print(\"User features shape:\", user_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o arquivo de preferencias e removendo as duas primeiras colunas de metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the preference order data manually\n",
    "preference_data_path = 'data/sushi3-2016/sushi3a.5000.10.order'\n",
    "with open(preference_data_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove the first row which contains metadata\n",
    "lines = lines[1:]\n",
    "\n",
    "# Split each line into a list of preferences\n",
    "preference_data = [line.strip().split() for line in lines]\n",
    "\n",
    "# Convert to a DataFrame\n",
    "preference_df = pd.DataFrame(preference_data)\n",
    "\n",
    "# Convert all values to integers\n",
    "preference_df = preference_df.astype(int)\n",
    "\n",
    "# Rename columns for clarity\n",
    "preference_df.columns = [f'pref_{i}' for i in range(preference_df.shape[1])]\n",
    "\n",
    "# Display the processed preference data\n",
    "\n",
    "# Remove the first two columns if they contain metadata\n",
    "preference_df = preference_df.drop(columns=['pref_0', 'pref_1'])\n",
    "\n",
    "# Rename columns for clarity\n",
    "preference_df.columns = [f'pref_{i}' for i in range(preference_df.shape[1])]\n",
    "\n",
    "# Display the cleaned preference data\n",
    "print(preference_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pairwise(preference_df):\n",
    "    pairwise_data = []\n",
    "    for user_id, preferences in preference_df.iterrows():\n",
    "        for i in range(len(preferences)):\n",
    "            for j in range(i + 1, len(preferences)):\n",
    "                item_i = preferences[i]\n",
    "                item_j = preferences[j]\n",
    "                pairwise_data.append([user_id, item_i, item_j])\n",
    "    pairwise_df = pd.DataFrame(pairwise_data, columns=['user_id', 'item_i', 'item_j'])\n",
    "    return pairwise_df\n",
    "\n",
    "# Convert the preference data to pairwise format\n",
    "pairwise_df = convert_to_pairwise(preference_df)\n",
    "\n",
    "# Display the processed pairwise preference data\n",
    "print(pairwise_df.head())\n",
    "print(pairwise_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "M=10\n",
    "\n",
    "#now subsample the pairwise data taking 5 pairs for each of the first 50 users\n",
    "pairwise_df = pairwise_df[pairwise_df['user_id'] < M]\n",
    "pairwise_df = pairwise_df.sample(k*M, random_state=0)\n",
    "\n",
    "#order df by user_id\n",
    "pairwise_df = pairwise_df.sort_values('user_id')\n",
    "pairwise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preference(user_idx, prod_1_idx, prod_2_idx):\n",
    "    user_pref = preference_df.loc[user_idx]\n",
    "\n",
    "    for i in range(len(user_pref)):\n",
    "        if user_pref.iloc[i] == prod_1_idx:\n",
    "            return 1\n",
    "        if user_pref.iloc[i] == prod_2_idx:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPPrefenceElicitation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, user_features, item_features, preference_data, max_iter=100):\n",
    "        self.user_features = user_features\n",
    "        self.item_features = item_features\n",
    "        self.preference_data = preference_data\n",
    "        self.n_users = user_features.shape[0]\n",
    "        self.n_items = item_features.shape[0]\n",
    "        self.max_iter=max_iter\n",
    "        self.maximum_hyperparameters(max_iter=self.max_iter)\n",
    "        user_covariance = self.kernel_covariance_matrix(user_features, user_features, self.sv_t, self.ls_t)\n",
    "        item_covariance = self.kernel_covariance_matrix(item_features, item_features, self.sv_k, self.ls_k)\n",
    "        self.covariance_matrix = torch.kron(user_covariance, item_covariance)\n",
    "        self.precision_matrix = torch.linalg.inv(self.covariance_matrix+torch.eye(self.n_users*self.n_items)*0.000001)\n",
    "    \n",
    "    def grad_log_p_D_given_f(self, f, preference_data):\n",
    "        f = torch.tensor(f, dtype=torch.float32)\n",
    "        grad = torch.zeros_like(f, dtype=torch.float32)\n",
    "        # Smoothing factor for numerical stability\n",
    "        eps = 1e-10\n",
    "        \n",
    "        for preference in preference_data:\n",
    "            t, k_1, k_2 = preference\n",
    "            index_1 = t * self.n_items + k_1\n",
    "            index_2 = t * self.n_items + k_2\n",
    "            z = (f[index_1] - f[index_2]) / self.noise_sd\n",
    "            phi_val = scipy.stats.norm.pdf(z)\n",
    "            Phi_val = scipy.stats.norm.cdf(z)\n",
    "            ratio = phi_val / (Phi_val * self.noise_sd + eps)\n",
    "            grad[index_1] += ratio\n",
    "            grad[index_2] -= ratio\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "    def hessian_log_p_D_given_f(self, f, preference_data):\n",
    "        hessian = torch.zeros((f.shape[0], f.shape[0]), dtype=torch.float32)\n",
    "        # Smoothing factor for numerical stability\n",
    "        eps = 1e-10\n",
    "        \n",
    "        for preference in preference_data:\n",
    "            t, k_1, k_2 = preference\n",
    "            index_1 = t * self.n_items + k_1\n",
    "            index_2 = t * self.n_items + k_2\n",
    "            z = (f[index_1] - f[index_2]) / self.noise_sd\n",
    "            phi_val = scipy.stats.norm.pdf(z)\n",
    "            Phi_val = scipy.stats.norm.cdf(z)\n",
    "            ratio = phi_val / (Phi_val * self.noise_sd + eps)\n",
    "            \n",
    "            second_derivative = (z * ratio + ratio**2) / (self.noise_sd ** 2)\n",
    "            hessian[index_1, index_1] -= second_derivative\n",
    "            hessian[index_2, index_2] -= second_derivative\n",
    "            hessian[index_1, index_2] += second_derivative\n",
    "            hessian[index_2, index_1] += second_derivative\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def maximum_a_posteriori(self, initial_f, preference_data, max_iter=100):\n",
    "        f = torch.tensor(initial_f, dtype=torch.float32)\n",
    "        # Check f vector for NaN\n",
    "        if torch.isnan(f).any():\n",
    "            raise ValueError(\"Initial f vector cannot contain NaN\")\n",
    "        for i in range(max_iter):\n",
    "            hessian = self.hessian_log_p_D_given_f(f, preference_data)\n",
    "            precision_matrix_post = hessian + torch.tensor(self.precision_matrix, dtype=torch.float32)\n",
    "            grad = self.grad_log_p_D_given_f(f, preference_data)\n",
    "            hessian_mult_f = hessian @ f\n",
    "            f = torch.linalg.solve(precision_matrix_post, grad + hessian_mult_f)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def se_kernel(self, x, y, sv, ls):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        if sv != sv or ls != ls:\n",
    "            raise ValueError(\"Hyperparameters cannot be NaN\")\n",
    "        return sv * torch.exp(-0.5 * torch.linalg.norm(x - y) ** 2 / ls ** 2)\n",
    "    \n",
    "    def kernel_covariance_matrix(self, x, y, sv, ls):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        n_x = x.shape[0]\n",
    "        n_y = y.shape[0]\n",
    "        covariance_matrix = torch.zeros((n_x, n_y), dtype=torch.float32)\n",
    "        \n",
    "        for i in range(n_x):\n",
    "            for j in range(n_y):\n",
    "                covariance_matrix[i, j] = self.se_kernel(x[i], y[j], sv, ls)\n",
    "        \n",
    "        return covariance_matrix\n",
    "\n",
    "    def maximum_hyperparameters(self, init_sv_t=1.0, init_ls_t=1.0, init_sv_k=1.0, init_ls_k=1.0, init_log_noise_sd=0.1, max_iter=100):\n",
    "        sv_t = torch.tensor(init_sv_t, requires_grad=True, dtype=torch.float32)\n",
    "        ls_t = torch.tensor(init_ls_t, requires_grad=True, dtype=torch.float32)\n",
    "        sv_k = torch.tensor(init_sv_k, requires_grad=True, dtype=torch.float32)\n",
    "        ls_k = torch.tensor(init_ls_k, requires_grad=True, dtype=torch.float32)\n",
    "        log_noise_sd = torch.tensor(init_log_noise_sd, requires_grad=True, dtype=torch.float32)\n",
    "        self.noise_sd = torch.exp(log_noise_sd)\n",
    "        optimizer = torch.optim.Adam([sv_t, ls_t, sv_k, ls_k, log_noise_sd])\n",
    "        identity = torch.eye(self.n_users * self.n_items, dtype=torch.float32)\n",
    "        for i in range(max_iter):\n",
    "            optimizer.zero_grad()\n",
    "            user_covariance = self.kernel_covariance_matrix(self.user_features, self.user_features, sv_t, ls_t)\n",
    "            item_covariance = self.kernel_covariance_matrix(self.item_features, self.item_features, sv_k, ls_k)\n",
    "            self.covariance_matrix = torch.kron(user_covariance, item_covariance)\n",
    "            self.precision_matrix = torch.linalg.inv(self.covariance_matrix + torch.eye(self.n_users * self.n_items, dtype=torch.float32) * 0.000001)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                f = self.maximum_a_posteriori(np.zeros(self.n_users * self.n_items), self.preference_data, max_iter=self.max_iter)\n",
    "                hessian = self.hessian_log_p_D_given_f(f, self.preference_data)\n",
    "            \n",
    "            f = f.reshape(1, -1)\n",
    "            sum_log_cdf = 0\n",
    "            for preference in self.preference_data:\n",
    "                t, k_1, k_2 = preference\n",
    "                index_1 = t * self.n_items + k_1\n",
    "                index_2 = t * self.n_items + k_2\n",
    "                sum_log_cdf += torch.log(torch.tensor(scipy.stats.norm.cdf((f[0, index_1] - f[0, index_2]) / self.noise_sd.detach().numpy()), dtype=torch.float32))\n",
    "            \n",
    "            loss = 0.5 * torch.logdet(self.covariance_matrix @ hessian + identity) + 0.5 * f @ self.precision_matrix @ f.T - sum_log_cdf\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        self.sv_t = sv_t.item()\n",
    "        self.ls_t = ls_t.item()\n",
    "        self.sv_k = sv_k.item()\n",
    "        self.ls_k = ls_k.item()\n",
    "        self.noise_sd = torch.exp(log_noise_sd).item()\n",
    "    \n",
    "    def predictive_mean_and_variance(self, f_hat, hessian, t_features, x_features):\n",
    "        k_t_star = torch.zeros(self.n_users, 1)\n",
    "        for i in range(self.n_users):\n",
    "            k_t_star[i] = self.se_kernel(t_features, self.user_features[i], self.sv_t, self.ls_t)\n",
    "        \n",
    "        k_x_star = torch.zeros(self.n_items, 1)\n",
    "        for i in range(self.n_items):\n",
    "            k_x_star[i] = self.se_kernel(x_features, self.item_features[i], self.sv_k, self.ls_k)\n",
    "        \n",
    "        kernel_t = self.se_kernel(t_features, t_features, self.sv_t, self.ls_t)\n",
    "        kernel_x = self.se_kernel(x_features, x_features, self.sv_k, self.ls_k)\n",
    "        Sigma_star = kernel_t * kernel_x\n",
    "        \n",
    "        k_star = torch.kron(k_t_star, k_x_star)\n",
    "        \n",
    "        predictive_mean = k_star.T @ self.precision_matrix @ f_hat\n",
    "        predictive_variance = Sigma_star - k_star.T @ torch.linalg.solve(self.covariance_matrix + torch.linalg.inv(hessian + torch.eye(self.n_users * self.n_items, dtype=torch.float32) * 0.000001), k_star)\n",
    "\n",
    "        return predictive_mean, predictive_variance\n",
    "\n",
    "    def joint_predictive_mean_and_variance(self, f_hat, hessian, t_features, x_1_features, x_2_features):\n",
    "        k_t_star = torch.zeros(self.n_users, 1)\n",
    "        for i in range(self.n_users):\n",
    "            k_t_star[i] = self.se_kernel(t_features, self.user_features[i], self.sv_t, self.ls_t)\n",
    "        \n",
    "        k_x_star = torch.zeros(self.n_items, 2)\n",
    "        for i in range(self.n_items):\n",
    "            k_x_star[i, 0] = self.se_kernel(x_1_features, self.item_features[i], self.sv_k, self.ls_k)\n",
    "            k_x_star[i, 1] = self.se_kernel(x_2_features, self.item_features[i], self.sv_k, self.ls_k)\n",
    "        \n",
    "        kernel_t = self.se_kernel(t_features, t_features, self.sv_t, self.ls_t)\n",
    "        Sigma_star = torch.zeros(2, 2)\n",
    "        Sigma_star[0, 0] = kernel_t * self.se_kernel(x_1_features, x_1_features, self.sv_k, self.ls_k)\n",
    "        Sigma_star[1, 1] = kernel_t * self.se_kernel(x_2_features, x_2_features, self.sv_k, self.ls_k)\n",
    "        Sigma_star[0, 1] = kernel_t * self.se_kernel(x_1_features, x_2_features, self.sv_k, self.ls_k)\n",
    "        Sigma_star[1, 0] = kernel_t * self.se_kernel(x_2_features, x_1_features, self.sv_k, self.ls_k)\n",
    "        \n",
    "        k_star = torch.kron(k_t_star, k_x_star)\n",
    "        \n",
    "        predictive_mean = k_star.T @ self.precision_matrix @ f_hat\n",
    "        predictive_variance = Sigma_star - k_star.T @ torch.linalg.solve(self.covariance_matrix + torch.linalg.inv(hessian + torch.eye(self.n_users * self.n_items, dtype=torch.float32) * 0.000001), k_star)\n",
    "        \n",
    "        return predictive_mean, predictive_variance\n",
    "    \n",
    "    def expected_improvement(self, t_features, x_features, f_hat, hessian, f_best=None):\n",
    "        if f_best is None:\n",
    "            f_best = torch.max(f_hat)\n",
    "        \n",
    "        predictive_mean, predictive_variance = self.predictive_mean_and_variance(f_hat, hessian, t_features, x_features)\n",
    "        \n",
    "        pred_sd = torch.sqrt(predictive_variance)\n",
    "        z_prime = (predictive_mean - f_best) / pred_sd\n",
    "        \n",
    "        return pred_sd * (z_prime * scipy.stats.norm.cdf(z_prime) + scipy.stats.norm.pdf(z_prime))\n",
    "    \n",
    "    def maximum_expected_improvement(self, t_features, item_features, f_hat, hessian, f_best=None):\n",
    "        maximum_ei = -np.inf\n",
    "        \n",
    "        for x in item_features:\n",
    "            ei = self.expected_improvement(t_features, x, f_hat, hessian, f_best)\n",
    "            if ei > maximum_ei:\n",
    "                maximum_ei = ei\n",
    "        \n",
    "        return maximum_ei\n",
    "    \n",
    "    def prob_i_over_j(self, t_features, x_1_index, x_2_index, f_hat, hessian):\n",
    "        predictive_mean, predictive_variance = self.joint_predictive_mean_and_variance(f_hat, hessian, t_features, self.item_features[x_1_index], self.item_features[x_2_index])\n",
    "        \n",
    "        numerator = predictive_mean[0] - predictive_mean[1]\n",
    "        denominator = predictive_variance[0, 0] - predictive_variance[1, 1] - 2 * predictive_variance[0, 1] - self.noise_sd ** 2\n",
    "        \n",
    "        return scipy.stats.norm.cdf(numerator / denominator)\n",
    "\n",
    "    def expected_value_of_information(self, t_index, x_1_index, x_2_index, f_hat, hessian, f_best=None):\n",
    "        t_features = self.user_features[t_index]\n",
    "        maximum_ei = self.maximum_expected_improvement(t_features, self.item_features, f_hat, hessian, f_best)\n",
    "        \n",
    "        preference_data_1_2 = np.append(self.preference_data, [[t_index, x_1_index, x_2_index]], axis=0)\n",
    "        f_hat_1_2 = self.maximum_a_posteriori(f_hat, preference_data_1_2)\n",
    "        hessian_1_2 = self.hessian_log_p_D_given_f(f_hat_1_2, preference_data_1_2)\n",
    "        \n",
    "        preference_data_2_1 = np.append(self.preference_data, [[t_index, x_2_index, x_1_index]], axis=0)\n",
    "        f_hat_2_1 = self.maximum_a_posteriori(f_hat, preference_data_2_1)\n",
    "        hessian_2_1 = self.hessian_log_p_D_given_f(f_hat_2_1, preference_data_2_1)\n",
    "        \n",
    "        mei_1_2 = self.maximum_expected_improvement(t_features, self.item_features, f_hat_1_2, hessian_1_2, f_best)\n",
    "        mei_2_1 = self.maximum_expected_improvement(t_features, self.item_features, f_hat_2_1, hessian_2_1, f_best)\n",
    "        \n",
    "        prob_1_2 = self.prob_i_over_j(t_features, x_1_index, x_2_index, f_hat, hessian)\n",
    "        prob_2_1 = 1 - prob_1_2\n",
    "        \n",
    "        return -maximum_ei + prob_1_2 * mei_1_2 + prob_2_1 * mei_2_1\n",
    "\n",
    "    def preference_elicitation(self, t_index, num_queries):\n",
    "        # Make a copy\n",
    "        pref_data = self.preference_data.copy()\n",
    "        f_hat = self.maximum_a_posteriori(np.zeros(self.n_users * self.n_items), self.preference_data)\n",
    "        hessian = self.hessian_log_p_D_given_f(f_hat, self.preference_data)\n",
    "        pred_hist = []\n",
    "        \n",
    "        candidate_pairs = []\n",
    "        for x_1 in range(self.n_items):\n",
    "            for x_2 in range(x_1 + 1, self.n_items):\n",
    "                found = False\n",
    "                for pref in pref_data:\n",
    "                    if pref[0] != t_index:\n",
    "                        continue\n",
    "                    if pref[1] == x_1 and pref[2] == x_2:\n",
    "                        found = True\n",
    "                        break\n",
    "                    if pref[1] == x_2 and pref[2] == x_1:\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    candidate_pairs.append((x_1, x_2))\n",
    "        \n",
    "        for i in range(num_queries):\n",
    "            max_evoi = -np.inf\n",
    "            for x_1, x_2 in candidate_pairs:\n",
    "                evoi = self.expected_value_of_information(t_index, x_1, x_2, f_hat, hessian)\n",
    "                if evoi > max_evoi:\n",
    "                    max_evoi = evoi\n",
    "                    max_x_1 = x_1\n",
    "                    max_x_2 = x_2\n",
    "            \n",
    "            candidate_pairs.remove((max_x_1, max_x_2))\n",
    "            \n",
    "            if get_preference(t_index, max_x_1, max_x_2):\n",
    "                np.append(pref_data, [[t_index, max_x_1, max_x_2]], axis=0)\n",
    "            else:\n",
    "                np.append(pref_data, [[t_index, max_x_2, max_x_1]], axis=0)\n",
    "            \n",
    "            f_hat = self.maximum_a_posteriori(f_hat, pref_data)\n",
    "            pred_hist.append(f_hat[t_index * self.n_items:(t_index + 1) * self.n_items])\n",
    "            hessian = self.hessian_log_p_D_given_f(f_hat, pref_data)\n",
    "        \n",
    "        return pred_hist\n",
    "\n",
    "# Example usage\n",
    "# model = GPPrefenceElicitation()\n",
    "# user_features = np.random.rand(50, 5)  # Replace with actual user features\n",
    "# item_features = np.random.rand(10, 5)  # Replace with actual item features\n",
    "# preference_data = np.array([[0, 1, 2], [1, 3, 4], [2, 5, 6]])  # Replace with actual preference data\n",
    "\n",
    "# model.fit(user_features, item_features, preference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_v=torch.tensor(user_features.values, dtype=torch.float32)\n",
    "item_features_v=torch.tensor(item_features.values, dtype=torch.float32)\n",
    "\n",
    "# model = GPPrefenceElicitation()\n",
    "# model.fit(user_features_v[:100], item_features_v, pairwise_df.values,max_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_df[\"item_j\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitua pelas suas variáveis reais\n",
    "# user_features_v = np.random.rand(50, 5)  # Exemplo de dados de características dos usuários\n",
    "# item_features_v = np.random.rand(10, 5)  # Exemplo de dados de características dos itens\n",
    "\n",
    "# # Exemplo de dados de preferências pareadas como DataFrame\n",
    "# pairwise_df = pd.DataFrame({\n",
    "#     'user_id': np.repeat(np.arange(50), 10),\n",
    "#     'item_1': np.tile(np.arange(10), 50),\n",
    "#     'item_2': np.tile((np.arange(10) + 1) % 10, 50)\n",
    "# })\n",
    "\n",
    "# Instancie seu modelo\n",
    "model = GPPrefenceElicitation()\n",
    "\n",
    "# Defina o número de divisões para a validação cruzada\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# Lista para armazenar as perdas normalizadas para diferentes números de consultas\n",
    "all_normalized_losses = []\n",
    "\n",
    "prediction_histories = {}\n",
    "\n",
    "iter = 0\n",
    "\n",
    "# Realize a validação cruzada\n",
    "for train_index, test_index in kf.split(pairwise_df['user_id'].unique()):\n",
    "    print(f\"Iteration {iter}\")\n",
    "    iter += 1\n",
    "    # Ajuste os índices para incluir todos os 10 pares de cada usuário\n",
    "    train_users = pairwise_df['user_id'].isin(train_index)\n",
    "    test_users = pairwise_df['user_id'].isin(test_index)\n",
    "\n",
    "    # Divida os dados em treino e teste\n",
    "    train_pairwise = pairwise_df[train_users].values\n",
    "    test_pairwise = pairwise_df[test_users].values\n",
    "    \n",
    "    # Ajuste o modelo aos dados de treino\n",
    "    model.fit(user_features_v[:M], item_features_v, train_pairwise, max_iter=3)\n",
    "        \n",
    "    # Realize a elicitação para o conjunto de teste\n",
    "    for t_index in test_index:\n",
    "        prediction_histories[t_index] = model.preference_elicitation(t_index, num_queries=20)\n",
    "\n",
    "# # Combine e analise os resultados\n",
    "# all_normalized_losses = np.array(all_normalized_losses)\n",
    "# mean_losses = np.mean(all_normalized_losses, axis=0)\n",
    "# std_losses = np.std(all_normalized_losses, axis=0)\n",
    "\n",
    "# for i, num_queries in enumerate(query_counts):\n",
    "#     print(f\"Mean normalized loss for {num_queries} queries:\", mean_losses[i])\n",
    "#     print(f\"Standard deviation of normalized loss for {num_queries} queries:\", std_losses[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
